#!/usr/bin/env python3
"""
Competition Miner - OPTIMIZED VERSION

Key optimizations:
- Pre-scoring duplicate filtering (saves 30-40% GPU time)
- Single InChIKey calculation per molecule
- Vectorized NumPy scoring (10-100x faster)
- Proper filtering order: dedup ‚Üí validate ‚Üí score
"""

import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

import sys
import json
import bittensor as bt
import pandas as pd
import numpy as np  # ‚Üê NEW: For vectorized scoring
from rdkit import Chem
from rdkit.Chem import Descriptors
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# Setup paths
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
PARENT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PARENT_DIR)

# Imports
import nova_ph2
from nova_ph2.PSICHIC.wrapper import PsichicWrapper
from nova_ph2.combinatorial_db.reactions import get_smiles_from_reaction
from nova_ph2.utils import get_heavy_atom_count
from smart_sampler import run_sampler, update_learner_with_scores

# Database path
DB_PATH = str(Path(nova_ph2.__file__).resolve().parent / "combinatorial_db" / "molecules.sqlite")

# Output paths
OUTPUT_PATH = "/output/result.json"
SAMPLER_PATH = "/tmp/sampler_file.json"


def get_config(input_file="/workspace/input.json"):
    """Get config from competition input file"""
    with open(input_file, "r") as f:
        d = json.load(f)
    config = {**d.get("config", {}), **d.get("challenge", {})}
    return config


def calculate_final_scores(
    score_dict: dict, 
    sampler_data: dict, 
    config: dict
) -> pd.DataFrame:
    """
    VECTORIZED score calculation using NumPy (10-100x faster)
    
    Formula: avg_target - (antitarget_weight √ó avg_antitarget)
    """
    names = sampler_data["molecules"]
    smiles = sampler_data["smiles"]
    
    # InChIKeys should already be calculated in pre-filter
    inchikey_list = sampler_data.get("inchikeys")
    if inchikey_list is None:
        # Fallback (shouldn't happen)
        inchikey_list = []
        for s in smiles:
            try:
                if s:
                    inchikey_list.append(Chem.MolToInchiKey(Chem.MolFromSmiles(s)))
                else:
                    inchikey_list.append(None)
            except Exception:
                inchikey_list.append(None)
    
    targets = score_dict['ps_target_scores']
    antitargets = score_dict['ps_antitarget_scores']
    
    # VECTORIZED SCORING with NumPy (10-100x faster!)
    try:
        target_array = np.asarray(targets, dtype=np.float32)  # shape: (n_target_models, n_mols)
        antitarget_array = np.asarray(antitargets, dtype=np.float32)  # shape: (n_antitarget_models, n_mols)
        
        avg_target = target_array.mean(axis=0) if target_array.size else np.zeros(len(names), dtype=np.float32)
        avg_antitarget = antitarget_array.mean(axis=0) if antitarget_array.size else np.zeros(len(names), dtype=np.float32)
        
        final_scores = (avg_target - (config["antitarget_weight"] * avg_antitarget)).tolist()
        
        bt.logging.info(f"‚úÖ Vectorized scoring completed ({len(names)} molecules)")
        
    except Exception as e:
        bt.logging.error(f"‚ö†Ô∏è Vectorized scoring failed, falling back to Python loops: {e}")
        # Fallback to Python loops
        final_scores = []
        for mol_idx in range(len(names)):
            try:
                target_scores_for_mol = [target_list[mol_idx] for target_list in targets] if targets else [0.0]
                avg_t = sum(target_scores_for_mol) / len(target_scores_for_mol)
                
                antitarget_scores_for_mol = [antitarget_list[mol_idx] for antitarget_list in antitargets] if antitargets else [0.0]
                avg_at = sum(antitarget_scores_for_mol) / len(antitarget_scores_for_mol)
                
                final_scores.append(avg_t - (config["antitarget_weight"] * avg_at))
            except Exception:
                final_scores.append(float('-inf'))
    
    # Create dataframe
    batch_scores = pd.DataFrame({
        "name": names,
        "smiles": smiles,
        "InChIKey": inchikey_list,
        "score": final_scores
    })
    
    return batch_scores


def iterative_sampling_loop(
    db_path: str,
    sampler_file_path: str,
    output_path: str,
    config: dict
) -> None:
    """
    Main mining loop - OPTIMIZED with pre-scoring dedup
    
    Process:
    1. Generate raw candidates (sampler)
    2. PRE-FILTER duplicates (calculate InChIKey once)
    3. Validate unique molecules only
    4. Score validated unique molecules (parallel PSICHIC)
    5. Calculate weighted scores (vectorized NumPy)
    6. Update elite learner
    7. Merge and write
    """
    
    # Initialize PSICHIC models once
    bt.logging.info(f"[Miner] üî¨ Initializing PSICHIC models...")
    target_models = []
    antitarget_models = []
    
    for seq in config["target_sequences"]:
        wrapper = PsichicWrapper()
        wrapper.initialize_model(seq)
        target_models.append(wrapper)
        bt.logging.info(f"[Miner]   ‚úì Loaded target model for {seq}")
    
    for seq in config["antitarget_sequences"]:
        wrapper = PsichicWrapper()
        wrapper.initialize_model(seq)
        antitarget_models.append(wrapper)
        bt.logging.info(f"[Miner]   ‚úì Loaded antitarget model for {seq}")
    
    bt.logging.info(f"[Miner] ‚úÖ Models ready: {len(target_models)} targets, {len(antitarget_models)} antitargets\n")
    
    n_samples = config["num_molecules"] * 5
    
    top_pool = pd.DataFrame(columns=["name", "smiles", "InChIKey", "score"])
    seen_inchikeys = set()  # Global duplicate tracker
    
    iteration = 0
    while True:
        iteration += 1
        bt.logging.info(f"\n{'='*70}")
        bt.logging.info(f"[Miner] üîÑ ITERATION {iteration}")
        bt.logging.info(f"{'='*70}")
        
        # STEP 1: Generate raw candidates
        bt.logging.info(f"[Miner] Generating {n_samples} raw candidates...")
        sampler_data = run_sampler(
            n_samples=n_samples,
            subnet_config=config,
            output_path=sampler_file_path,
            save_to_file=True,
            db_path=db_path,
        )
        
        if not sampler_data or not sampler_data.get("molecules"):
            bt.logging.warning("[Miner] ‚ö†Ô∏è No valid molecules produced; continuing")
            continue
        
        raw_count = len(sampler_data["molecules"])
        bt.logging.info(f"[Miner] ‚úÖ Generated {raw_count} raw candidates")
        
        # STEP 2: PRE-FILTER DUPLICATES (before validation!)
        bt.logging.info(f"[Miner] üîç Pre-filtering duplicates...")
        
        filtered_names = []
        filtered_smiles = []
        filtered_inchikeys = []
        
        for name, smile in tqdm(zip(sampler_data["molecules"], sampler_data["smiles"]), 
                               total=len(sampler_data["molecules"]),
                               desc="Filtering duplicates"):
            if not name or not smile:
                continue
            
            try:
                # Calculate InChIKey ONCE
                mol = Chem.MolFromSmiles(smile)
                if not mol:
                    continue
                
                key = Chem.MolToInchiKey(mol)
                
                # Check if already seen
                if key in seen_inchikeys:
                    continue  # ‚Üê SKIP EARLY! No validation, no scoring
                
                # This is unique - add it
                filtered_names.append(name)
                filtered_smiles.append(smile)
                filtered_inchikeys.append(key)
                seen_inchikeys.add(key)
                
            except Exception:
                continue
        
        unique_count = len(filtered_names)
        duplicates_found = raw_count - unique_count
        bt.logging.info(f"[Miner] ‚úÖ Pre-filter complete: {unique_count} unique, {duplicates_found} duplicates removed")
        
        if not filtered_names:
            bt.logging.warning("[Miner] ‚ö†Ô∏è No unique molecules after dedup; continuing")
            continue
        
        # STEP 3: Validate ONLY unique molecules
        bt.logging.info(f"[Miner] üî¨ Validating {unique_count} unique molecules...")
        
        validated_names = []
        validated_smiles = []
        validated_inchikeys = []
        
        for name, smile, key in zip(filtered_names, filtered_smiles, filtered_inchikeys):
            try:
                # Check heavy atoms
                if get_heavy_atom_count(smile) < config.get('min_heavy_atoms', 10):
                    continue
                
                # Check rotatable bonds
                mol = Chem.MolFromSmiles(smile)
                if not mol:
                    continue
                
                num_rot = Descriptors.NumRotatableBonds(mol)
                if num_rot < config.get('min_rotatable_bonds', 0) or \
                   num_rot > config.get('max_rotatable_bonds', 50):
                    continue
                
                validated_names.append(name)
                validated_smiles.append(smile)
                validated_inchikeys.append(key)
                
            except Exception:
                continue
        
        validated_count = len(validated_names)
        filtered_by_validation = unique_count - validated_count
        bt.logging.info(f"[Miner] ‚úÖ Validation complete: {validated_count} valid, {filtered_by_validation} filtered out")
        
        if not validated_names:
            bt.logging.warning("[Miner] ‚ö†Ô∏è No valid molecules after validation; continuing")
            continue
        
        # STEP 4: Score ONLY validated unique molecules (parallel PSICHIC)
        bt.logging.info(f"[Miner] üî¨ Scoring {validated_count} validated unique molecules (parallel)...")
        
        def score_with_model(model, smiles_list):
            """Helper function for parallel scoring"""
            try:
                res = model.score_molecules(smiles_list)
                return res['predicted_binding_affinity'].tolist()
            except Exception as e:
                bt.logging.error(f"[Miner] Model scoring failed: {e}")
                return [0.0] * len(smiles_list)
        
        # Parallel scoring of targets
        all_target_results = []
        with ThreadPoolExecutor(max_workers=max(1, len(target_models))) as executor:
            futures = {executor.submit(score_with_model, m, validated_smiles): idx 
                      for idx, m in enumerate(target_models)}
            for fut in as_completed(futures):
                all_target_results.append(fut.result())
        
        # Parallel scoring of antitargets
        all_antitarget_results = []
        with ThreadPoolExecutor(max_workers=max(1, len(antitarget_models))) as executor:
            futures = {executor.submit(score_with_model, m, validated_smiles): idx 
                      for idx, m in enumerate(antitarget_models)}
            for fut in as_completed(futures):
                all_antitarget_results.append(fut.result())
        
        score_dict = {
            'ps_target_scores': all_target_results,
            'ps_antitarget_scores': all_antitarget_results,
        }
        
        bt.logging.info(f"[Miner] ‚úÖ Parallel scoring complete")
        
        # STEP 5: Calculate final weighted scores (VECTORIZED)
        sampler_data_scored = {
            "molecules": validated_names,
            "smiles": validated_smiles,
            "inchikeys": validated_inchikeys  # Pass pre-calculated keys
        }
        
        batch_scores = calculate_final_scores(score_dict, sampler_data_scored, config)
        
        # Filter out invalid molecules
        batch_scores = batch_scores[batch_scores['name'].notna()]
        batch_scores = batch_scores[batch_scores['score'] > float('-inf')]
        
        if batch_scores.empty:
            bt.logging.warning("[Miner] ‚ö†Ô∏è No valid scores; continuing")
            continue
        
        bt.logging.info(f"[Miner] üìä Scored {len(batch_scores)} molecules:")
        bt.logging.info(f"         Best score: {batch_scores['score'].max():.6f}")
        bt.logging.info(f"         Mean score: {batch_scores['score'].mean():.6f}")
        
        # STEP 6: Teach the learner
        bt.logging.info(f"[Miner] üß† Teaching learner with scores...")
        molecules_with_scores = [
            (row['name'], row['score']) 
            for _, row in batch_scores.iterrows()
        ]
        update_learner_with_scores(molecules_with_scores)
        
        # STEP 7: Merge with existing pool
        top_pool = pd.concat([top_pool, batch_scores], ignore_index=True)
        
        # Sort by score BEFORE final deduplication
        top_pool = top_pool.sort_values(by="score", ascending=False)
        
        # Final dedup (shouldn't find any, but safety check)
        initial_pool_size = len(top_pool)
        top_pool = top_pool.drop_duplicates(subset=["InChIKey"], keep="first")
        final_dedup_count = initial_pool_size - len(top_pool)
        
        if final_dedup_count > 0:
            bt.logging.warning(f"[Miner] ‚ö†Ô∏è Found {final_dedup_count} late duplicates (shouldn't happen!)")
        
        # Keep top N
        top_pool = top_pool.head(config["num_molecules"])
        
        # STEP 8: Write to output
        top_entries = {"molecules": top_pool["name"].tolist()}
        
        with open(output_path, "w") as f:
            json.dump(top_entries, f, ensure_ascii=False, indent=2)
        
        bt.logging.info(f"[Miner] ‚úÖ ITERATION {iteration} COMPLETE:")
        bt.logging.info(f"         Pipeline: {raw_count} raw ‚Üí {unique_count} unique ‚Üí {validated_count} valid ‚Üí {len(batch_scores)} scored")
        bt.logging.info(f"         Efficiency: {(validated_count/raw_count)*100:.1f}% molecules actually scored")
        bt.logging.info(f"         Wrote {len(top_entries['molecules'])} molecules to {output_path}")
        bt.logging.info(f"         Top pool best: {top_pool['score'].max():.6f}")
        bt.logging.info(f"         Top pool mean: {top_pool['score'].mean():.6f}")


def main(config: dict):
    """Main entry point"""
    bt.logging.info("\n" + "="*70)
    bt.logging.info("üöÄ OPTIMIZED COMPETITION MINER")
    bt.logging.info("="*70)
    bt.logging.info(f"üìã Configuration:")
    bt.logging.info(f"   Target proteins: {config.get('target_sequences', [])}")
    bt.logging.info(f"   Antitarget proteins: {config.get('antitarget_sequences', [])}")
    bt.logging.info(f"   Allowed reaction: {config.get('allowed_reaction', 'unknown')}")
    bt.logging.info(f"   Target molecules: {config.get('num_molecules', 100)}")
    bt.logging.info(f"   Antitarget weight: {config.get('antitarget_weight', 0.9)}")
    bt.logging.info(f"\nüîß OPTIMIZATIONS ENABLED:")
    bt.logging.info(f"   ‚úÖ Pre-scoring duplicate filtering")
    bt.logging.info(f"   ‚úÖ Single InChIKey calculation")
    bt.logging.info(f"   ‚úÖ NumPy vectorized scoring")
    bt.logging.info(f"   ‚úÖ Simplified elite strategy")
    bt.logging.info("="*70 + "\n")
    
    iterative_sampling_loop(
        db_path=DB_PATH,
        sampler_file_path=SAMPLER_PATH,
        output_path=OUTPUT_PATH,
        config=config
    )


if __name__ == "__main__":
    config = get_config()
    main(config)
